{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLHcLz5zvGMH"
   },
   "source": [
    "# NLP From Scratch: Translation with a Seq2Seq RNN model\n",
    "\n",
    "In this project we will be teaching a neural network to translate from German to English.\n",
    "\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence\n",
    "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs-8uyBGvGMP"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttoEcLnfvGMR"
   },
   "outputs": [],
   "source": [
    "# using Python 3.9\n",
    "%pip install pandas torch matplotlib numpy ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i381hyrevGMT"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULBf_g5pvGMV"
   },
   "source": [
    "## Loading & preprocessing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3FBrubJvGMd"
   },
   "source": [
    "Implement the function `read_txt` to read the input file and transform it into a dataframe, which is then fed into the `parse_data` function in order to be normalized. If the parameter reverse is True, the order of languages in the tuple are reversed resulting in reverse translation. Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll only select a subset of pairs: Implement the `filter_pairs` function to drop all pairs where there is no word ending by a predefined list of suffixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_GJNRPYvGMg"
   },
   "outputs": [],
   "source": [
    "def read_txt(path:str)-> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  #TODO: Task 1 (5 points)\n",
    "  Parse the data from the file and return a DataFrame with columns ['ENG','GER'].\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Task 1 not implemented\")\n",
    "  return pairs\n",
    "\n",
    "\n",
    "\n",
    "def parse_data(pairs:pd.DataFrame, reverse=False)-> pd.DataFrame:\n",
    "  pairs['GER'] = pairs['GER'].apply(normalize_string)\n",
    "  pairs['ENG'] = pairs['ENG'].apply(normalize_string)\n",
    "\n",
    "  if reverse:\n",
    "    pairs = pairs.iloc[:, [1,0]]\n",
    "\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSAya9DovGMk"
   },
   "outputs": [],
   "source": [
    "suffixes = [\"hood\", \"ness\", \"ment\", \"ship\", \"ance\", \"ise\", \"ize\", \"ly\", \"etion\", \"ity\"]\n",
    "\n",
    "\"\"\"\n",
    "#TODO: Task 2 (10 pt)\n",
    "\n",
    "Implement the filter_pairs function that it takes in a pd.DataFrame of pairs of sentences in 2 languages \n",
    "and only selects the rows for which in the sentence of the selected language, there is at least one word ending\n",
    "by one of the suffices. E.g.:\n",
    "\n",
    "\"Sisterhood is very important\" --> keep\n",
    "\"We use Mentimeter in lectures --> drop\n",
    "\"Kindness and perseverance are virtues\" --> keep\n",
    "\n",
    "\n",
    "- those where one of both exceeds the maximal sentence length (MAX_WORDS)\n",
    "- those where the pair contains any of the words defined in interrogative_words or a question mark.\n",
    "\n",
    "Your method should work by pattern detection instead than explicit iteration.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def filter_pairs(pairs,\n",
    "                 suffixes,\n",
    "                 language=\"ENG\"):\n",
    "    raise NotImplementedError(\"Task 2 not implemented\")\n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ou8g3IGvGMX"
   },
   "source": [
    "Each word in a language will be represented as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfCoiIMHvGMY"
   },
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Language`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` which will be used to replace rare words later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfuEobYvvGMb"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\"\"\"\n",
    "TODO: Task 3 (10 pt)\n",
    "\n",
    "Implement the method stem, that takes as input a list of suffixes and maps all the words \n",
    "that can be created with a common stem + one of the suffixes in the list to a common word stem-\n",
    " that removes words whose frequency is below\n",
    "min_freq and makes sure that the size of the vocabulary is below max_vocab_size\n",
    "by iteratively removing least frequent words. \n",
    "\n",
    "The function should also take care of:\n",
    "- removing the original words from all counters and class dictionaries\n",
    "- updating word2count to the total counts for all the stemmed words\n",
    "- updating the index in the dictionary so that its values run from 0 to n_words\n",
    "\n",
    "You can create additional function for the task. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Language:\n",
    "    def __init__(self):\n",
    "        self.word2index = {} # maps word to integer index\n",
    "        self.word2count = {} # maps word to its frequency\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # maps index to a word\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "    def remove_word(self, word):\n",
    "        if word in self.word2index:\n",
    "            del self.word2count[word]\n",
    "            index = self.word2index[word]\n",
    "            del self.word2index[word]\n",
    "            del self.index2word[index]\n",
    "\n",
    "    def stem(self, suffix_list):\n",
    "        raise NotImplementedError(\"Task 3 not implemented\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyGhDql9vGMl"
   },
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5LZIJ9yvGMl"
   },
   "outputs": [],
   "source": [
    "def prepare_data(pairs, suffixes, stem=None):\n",
    "\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filter_pairs(pairs, suffixes=suffixes)\n",
    "    print(f\"Filtered {len(pairs)} sentence pairs\")\n",
    "    pairs = pairs.to_numpy()\n",
    "\n",
    "    input_lang = Language()\n",
    "    output_lang = Language()\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    \n",
    "    if stem:\n",
    "        (input_lang.stem(suffixes) if stem==\"input\"\n",
    "         else output_lang.stem(suffixes))\n",
    "\n",
    "    print(f\"Input language: {input_lang.n_words} words\")\n",
    "    print(f\"Output language: {output_lang.n_words} words\")\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "path = \"data/pairs.txt\"\n",
    "suffixes = [\"hood\", \"ness\", \"ment\", \"ship\", \"ance\", \"ise\", \"ize\", \"ly\", \"etion\", \"ity\"]\n",
    "pairs = parse_data(read_txt(path), reverse=True)\n",
    "input_lang, output_lang, pairs = prepare_data(pairs, suffixes, stem=\"output\")\n",
    "### SHOW NOTEBOOK OUTPUT ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XTWdb-uvGMp"
   },
   "source": [
    "\n",
    "## Preparing Training Data\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will add the SOS token at the beginngin and the\n",
    "EOS token at the end of both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4bSmXByOvGMp"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.insert(0, SOS_token)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8tTVtV7vGMq"
   },
   "source": [
    "### Seq2Seq RNN Model\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the `<SOS>` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state. At each next iteration, the decoder makes a prediction \n",
    "based on the most likely token predicted in the previous step. If \n",
    "\"teacher forcing\" is used, real target outputs are used as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but [when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Task 4 (10 pt)\n",
    "Implement the decoder's iterative step to accommodate for a probabilistic usage of teacher forcing according to `teacher_forcing_ratio`.\n",
    "At each step, the model should choose according to the ratio wheter to use its last prediction or the target token as an input for the next prediction. \n",
    "\n",
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: (batch_size, src_len)\n",
    "        embedded = self.embedding(src)  # (batch_size, src_len, embed_dim)\n",
    "        outputs, hidden = self.gru(embedded)  # outputs: (batch_size, src_len, hidden_dim), hidden: (n_layers, batch_size, hidden_dim)\n",
    "        return hidden  # Only hidden state is returned for decoder\n",
    "\n",
    "# Decoder Model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, trg, hidden):\n",
    "        trg = trg.unsqueeze(1)  \n",
    "        embedded = self.embedding(trg)  \n",
    "        output, hidden = self.gru(embedded, hidden)  \n",
    "        prediction = self.fc_out(output.squeeze(1))  \n",
    "        return prediction, hidden\n",
    "\n",
    "# Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: (1, src_len), trg: (1, trg_len)\n",
    "        trg_len = trg.size(1)\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(1, trg_len, output_dim).to(device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        hidden = self.encoder(src)\n",
    "\n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # Decode one token at a time\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            raise NotImplementedError(\"Task 4 not implemented\")\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkR7FiesvGMr"
   },
   "outputs": [],
   "source": [
    "cfg = {\"n_iters\": 10**3,\n",
    "       \"print_every\":10, \n",
    "       \"plot_every\":100,\n",
    "       \"learning_rate\":0.01, \n",
    "       \"teacher_forcing_ratio\":.5}\n",
    "\n",
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train(training_pair, \n",
    "          model,\n",
    "          optimizer, \n",
    "          criterion, \n",
    "          teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    input_tensor, target_tensor = training_pair\n",
    "    pred_tensor = model(input_tensor, target_tensor, teacher_forcing_ratio)\n",
    "    loss = criterion(pred_tensor.squeeze(0), target_tensor.squeeze(0))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item() \n",
    "\n",
    "\n",
    "def trainIters(model, cfg):\n",
    "\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                          lr=cfg[\"learning_rate\"])\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(cfg[\"n_iters\"])]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, cfg[\"n_iters\"] + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        loss = train(training_pair, \n",
    "                     model,\n",
    "                     optimizer, \n",
    "                     criterion,\n",
    "                     cfg[\"teacher_forcing_ratio\"]\n",
    "                     )\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % cfg[\"print_every\"] == 0:\n",
    "            print_loss_avg = print_loss_total / cfg[\"print_every\"]\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / cfg[\"n_iters\"]),\n",
    "                                         iter, iter / cfg[\"n_iters\"] * 100, print_loss_avg))\n",
    "\n",
    "        if iter % cfg[\"plot_every\"] == 0:\n",
    "            plot_loss_avg = plot_loss_total / cfg[\"plot_every\"]\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "    return plot_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of7lko38vGMt"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "joGj-tUNvGMt"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, sentence):\n",
    "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "    with torch.no_grad():\n",
    "        pred_tensor = model(input_tensor, input_tensor, teacher_forcing_ratio=0)\n",
    "        pred_indices = pred_tensor.squeeze(0).argmax(1).cpu().numpy()\n",
    "\n",
    "    pred_sentence = ' '.join([output_lang.index2word[i] for i in pred_indices])\n",
    "        \n",
    "    return pred_sentence\n",
    "\n",
    "#We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements\n",
    "def evaluateRandomly(model, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_sentence = evaluate(model, pair[0])\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsYC2ekZvGMu"
   },
   "source": [
    "## Training and Evaluating\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. When training from scratch after about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvtS8nxr6CTH"
   },
   "outputs": [],
   "source": [
    "\n",
    "decoder = Decoder(output_lang.n_words, 256, 512)\n",
    "encoder = Encoder(input_lang.n_words, 256, 512)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "losses = trainIters(model, cfg)\n",
    "### TODO: SHOW NOTEBOOK OUTPUT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: print outcome of trained model \n",
    " \n",
    "evaluateRandomly(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZlnoQBPvGMy"
   },
   "source": [
    "# Transfer learning\n",
    "## Fine tune out-of-the box encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C9JKpsPvGMz"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Task 5 (15 pt)\n",
    "\n",
    "Load a transformer (seq2seq model using attention layers) and fine-tune it to your problem.\n",
    "Show the training progress of the model and the training metrics. Briefly\n",
    "explain (3-4 sentences) the model choice and comment on the outcomes.\n",
    "\n",
    "Note: you don't have to use the above-defined methods for training.\n",
    "Splitting the dataset in train and test is welcomed, but not required for the task.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "raise NotImplementedError(\"Task 5 not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEXE_Wa8vGMM"
   },
   "source": [
    "# Credits\n",
    "\n",
    "This problem set is based upon an official PyTorch [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). Many thanks to PyTorch, [Sean Robertson](https://github.com/spro/practical-pytorch) and  [Florian Nachtigall](https://github.com/FlorianNachtigall).\n",
    "\n",
    "Be cautious with looking in the original notebook for answers. Many details have been changed and you won't be able to copy-and-paste solutions.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
